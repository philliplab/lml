# ISLR 3 - Linear Regression

## Lab

### Simple Linear Regression

```{r}
library(MASS)
library(ggplot2)
library(dplyr)
library(tidyr)
summary(Boston)
```
Dimensions:
```{r}
dim(Boston)
```
There are a decent number of rows for the number of columns, so we can consider building a large model.

Missing Values:
```{r}
sum(is.na(Boston))
```
There are no missing values. This is suspicious, there are always missing values. Check the data values to check for sentinel values that might indicate missingness.

Histograms:
```{r}
lBoston <- Boston %>% pivot_longer(everything())
ggplot(lBoston, aes(x = value)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_histogram()
```

- One binary variable: chas.
- There are a high-leverage points in crom and dis.
- The response variable looks like two different distributions: a normal centered around 20 and another distribution that peaks sharply at about 50.
  - We might want to use different models for these two groups.
- Many of the predictor distributions are very skewed leading to very different leverage for the different observations.
- The `rad` variable is semi-categorical and has two clearly different distributions. We should be careful about predicting for observations with `rad` values between 10 and 22.

Scatter plots and simple regression for continuous variables:
```{r}
l2Boston <- Boston %>% pivot_longer(crim:lstat) %>% filter(name != 'chas')
ggplot(l2Boston, aes(x = value, y = medv)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_point() +
  geom_smooth(method='lm')
```

All the variables seem like good candidate predictors. The relationships between the predictors and the response do not exhibit ideal patterns, but it is not easy to pick transformations that will improve the linearity of the relationships:
- 1/x for indus and lstat?
- square root/log for black and dis?

Boxplots for categorical variables:
```{r}
l3Boston <- Boston %>% pivot_longer(crim:lstat) %>% filter(name %in% c('chas', 'rad'))
ggplot(l3Boston, aes(x = factor(value), y = medv)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_boxplot()
```

- `chas` seems like a good and well-behaved predictor
- For values fo `rad` that are smaller than 8 the relationship seems flat or perhaps increasing with rad. However, for 24 it is clearly lower than for value smaller than 8. It appears that the nature of the relationship changes somewhere between 8 and 24 and we have no information about that. We might want to add a `rad^2` term.

Checking for correlations in the predictors:
```{r}
cBoston <- data.frame(round(cor(Boston), 2))
cBoston <- cBoston[row.names(cBoston) != 'medv', names(cBoston) != 'medv']
cBoston$metric1 <- row.names(cBoston)
cBoston <- cBoston %>% pivot_longer(crim:lstat)

cBoston <- cBoston %>% 
  filter(metric1 > name) %>%
  arrange(-abs(value))
cBoston$cor_rank <- 1:nrow(cBoston)

hist(cBoston$value)

#ggplot(cBoston, aes(x = metric1, y = name, fill = value)) +
#  geom_tile() +
#  scale_fill_gradient(low = 'blue', high = 'red')
```
There is strong correlation between predictors. This will cause collinearity problems.

Top 9 correlations between predictors:
```{r}
all_dat <- NULL
cBoston
par(mfrow=c(3,3))
i <- 1
for (i in 1:9){
  var1 <- cBoston$metric1[i]
  var2 <- cBoston$name[i]
  corr <- cBoston$value[i]
  all_dat <- rbind(all_dat, 
  data.frame(v1 = Boston[, var1], v2 = Boston[, var2], 
             corr = corr, 
             lab = paste0('V1: ', var1, ' & V2: ', var2),
             stringsAsFactors = FALSE))
}
print(all_dat)
ggplot(all_dat, aes(x = v1, y = v2))+
  facet_wrap(. ~ lab, scale = 'free') + 
  geom_point()
```

Note that nox, indus, tax and dis seems to all be correlated strongly with each other - we will probably want to drop some of these variables, or combine them into one predictor.

Fit simple models and investigate the best ones:
```{r}
simple_mods <- NULL
for (pred_name in names(Boston)[-ncol(Boston)]){
  mod <- lm(paste0('medv ~ ', pred_name), dat = Boston)
  print(summary(mod))
  simple_mods <- rbind(simple_mods,
    data.frame(pred = pred_name,
               intercept = round(mod$coefficients[1], 2),
               slope = round(mod$coefficients[2], 2),
               r2 = round(summary(mod)$r.squared, 2))
  )
}
knitr::kable(simple_mods[order(simple_mods$r2),])
```

Two predictors are clearly much better than the others:
- lstat
- rm

Basic fitting and dropping predictors with high p-values manually
```{r}
mod <- lm(medv ~ lstat + crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black, data = Boston)
summary(mod)
mod1 <- update(mod, . ~ . - indus)
summary(mod1)
mod2 <- update(mod1, . ~ . - age)
summary(mod2)
names(mod2$coefficients)
```

Let's track how the coefficients change as variables are dropped. We start with the initial full model on the full dataset as baseline and then report the ratios relative to that.
```{r}
coef_names <- names(mod2$coefficients)
ct <-  # ct is short for coefficient tracker
data.frame(
  name = c(coef_names, 'adjR2', 'Fstat') ,
  m00 = round(c(mod$coefficients[coef_names],
                summary(mod)$adj.r.squared,
                summary(mod)$fstatistic[1]),
              3)
)

get_mod_stats <- function(mod_var){
  mod_coef <- mod_var$coefficients[coef_names]
  mod_coef_norm <- mod_coef / ct$m00[ct$name %in% coef_names]
  round(c(mod_coef_norm,
          summary(mod_var)$adj.r.squared,
          summary(mod_var)$fstatistic[1]),
        3)
}

ct$m01 <- get_mod_stats(mod1) 
ct$m02 <- get_mod_stats(mod2)

row.names(ct) <- NULL
print(ct)
```

Summary of model with only significant predictors.
```{r}
print(summary(mod2))
```
The model fits the data very well and explains a high degree of the variation of the data (adj. R2 = 73%).

```{r}
confint(mod2)
```


```{r}
knitr::kable(ct)
```





## Exercises

### Applied
