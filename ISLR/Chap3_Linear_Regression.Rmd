# ISLR 3 - Linear Regression

## Lab

### Simple Linear Regression

```{r}
library(MASS)
library(ggplot2)
library(dplyr)
library(tidyr)
library(car)
summary(Boston)
```
Dimensions:
```{r}
dim(Boston)
```
There are a decent number of rows for the number of columns, so we can consider building a large model.

Missing Values:
```{r}
sum(is.na(Boston))
```
There are no missing values. This is suspicious, there are always missing values. Check the data values to check for sentinel values that might indicate missingness.

Histograms:
```{r}
lBoston <- Boston %>% pivot_longer(everything())
ggplot(lBoston, aes(x = value)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_histogram()
```

- One binary variable: chas.
- There are a high-leverage points in crom and dis.
- The response variable looks like two different distributions: a normal centered around 20 and another distribution that peaks sharply at about 50.
  - We might want to use different models for these two groups.
- Many of the predictor distributions are very skewed leading to very different leverage for the different observations.
- The `rad` variable is semi-categorical and has two clearly different distributions. We should be careful about predicting for observations with `rad` values between 10 and 22.

Scatter plots and simple regression for continuous variables:
```{r}
l2Boston <- Boston %>% pivot_longer(crim:lstat) %>% filter(name != 'chas')
ggplot(l2Boston, aes(x = value, y = medv)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_point() +
  geom_smooth(method='lm')
```

All the variables seem like good candidate predictors. The relationships between the predictors and the response do not exhibit ideal patterns, but it is not easy to pick transformations that will improve the linearity of the relationships:
- 1/x for indus and lstat?
- square root/log for black and dis?

Boxplots for categorical variables:
```{r}
l3Boston <- Boston %>% pivot_longer(crim:lstat) %>% filter(name %in% c('chas', 'rad'))
ggplot(l3Boston, aes(x = factor(value), y = medv)) +
  facet_wrap(. ~ name, scales = 'free') +
  geom_boxplot()
```

- `chas` seems like a good and well-behaved predictor
- For values fo `rad` that are smaller than 8 the relationship seems flat or perhaps increasing with rad. However, for 24 it is clearly lower than for value smaller than 8. It appears that the nature of the relationship changes somewhere between 8 and 24 and we have no information about that. We might want to add a `rad^2` term.

Checking for correlations in the predictors:
```{r}
cBoston <- data.frame(round(cor(Boston), 2))
cBoston <- cBoston[row.names(cBoston) != 'medv', names(cBoston) != 'medv']
cBoston$metric1 <- row.names(cBoston)
cBoston <- cBoston %>% pivot_longer(crim:lstat)

cBoston <- cBoston %>% 
  filter(metric1 > name) %>%
  arrange(-abs(value))
cBoston$cor_rank <- 1:nrow(cBoston)

hist(cBoston$value)

#ggplot(cBoston, aes(x = metric1, y = name, fill = value)) +
#  geom_tile() +
#  scale_fill_gradient(low = 'blue', high = 'red')
```
There is strong correlation between predictors. This will cause collinearity problems.

Top 9 correlations between predictors:
```{r}
all_dat <- NULL
cBoston
i <- 1
for (i in 1:9){
  var1 <- cBoston$metric1[i]
  var2 <- cBoston$name[i]
  corr <- cBoston$value[i]
  all_dat <- rbind(all_dat, 
  data.frame(v1 = Boston[, var1], v2 = Boston[, var2], 
             corr = corr, 
             lab = paste0('V1: ', var1, ' & V2: ', var2),
             stringsAsFactors = FALSE))
}
print(all_dat)
ggplot(all_dat, aes(x = v1, y = v2))+
  facet_wrap(. ~ lab, scale = 'free') + 
  geom_point()
```

Note that nox, indus, tax and dis seems to all be correlated strongly with each other - we will probably want to drop some of these variables, or combine them into one predictor.

Fit simple models and investigate the best ones:
```{r}
simple_mods <- NULL
for (pred_name in names(Boston)[-ncol(Boston)]){
  mod <- lm(paste0('medv ~ ', pred_name), dat = Boston)
  print(summary(mod))
  simple_mods <- rbind(simple_mods,
    data.frame(pred = pred_name,
               intercept = round(mod$coefficients[1], 2),
               slope = round(mod$coefficients[2], 2),
               r2 = round(summary(mod)$r.squared, 2))
  )
}
knitr::kable(simple_mods[order(simple_mods$r2),])
```

Two predictors are clearly much better than the others:
- lstat
- rm

lstat
```{r}
mod <- lm(medv ~ lstat, dat = Boston)
plot(medv ~ lstat, dat = Boston)
abline(mod)
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
plot(hatvalues(mod) ~ Boston$medv)
```

Non-linear relationship - try sqrt(1/x) transformation:

```{r}
mod <- lm(medv ~ I((1/lstat)^(1/2)), dat = Boston)
plot(medv ~ I((1/lstat)^(1/2)), dat = Boston)
abline(mod)
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
plot(hatvalues(mod) ~ I((1/Boston$lstat)^(1/2)))
```
Much better. A few moderately high leverage data points.

rm
```{r}
mod <- lm(medv ~ rm, dat = Boston)
plot(medv ~ rm, dat = Boston)
abline(mod)
print(summary(mod))
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
plot(hatvalues(mod) ~ Boston$rm)
```

Looks decent, but a couple of clear outliers.
```{r}
rmOutliers <- Boston[!abs(mod$residuals)>18,]
mod <- lm(medv ~ rm, dat = rmOutliers)
plot(medv ~ rm, dat = rmOutliers)
abline(mod)
print(summary(mod))
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
```

Removing just those couple of outlier has a large effect, should check with an expert if they might represent special cases.

Basic fitting and dropping predictors with high p-values manually
```{r}
mod <- lm(medv ~ lstat + crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black, data = Boston)
mod1 <- update(mod, . ~ . - indus)
mod2 <- update(mod1, . ~ . - age)
```

Let's track how the coefficients change as variables are dropped. We start with the initial full model on the full dataset as baseline and then report the ratios relative to that.
```{r}
coef_names <- names(mod2$coefficients)
ct <-  # ct is short for coefficient tracker
data.frame(
  name = c(coef_names, 'adjR2', 'Fstat') ,
  m00 = round(c(mod$coefficients[coef_names],
                summary(mod)$adj.r.squared,
                summary(mod)$fstatistic[1]),
              3)
)

get_mod_stats <- function(mod_var){
  mod_coef <- mod_var$coefficients[coef_names]
  mod_coef_norm <- mod_coef / ct$m00[ct$name %in% coef_names]
  round(c(mod_coef_norm,
          summary(mod_var)$adj.r.squared,
          summary(mod_var)$fstatistic[1]),
        3)
}

ct$m01 <- get_mod_stats(mod1) 
ct$m02 <- get_mod_stats(mod2)

row.names(ct) <- NULL
```

Summary of model with only significant predictors.
```{r}
print(summary(mod2))
```
The model fits the data very well and explains a high degree of the variation of the data (adj. R2 = 73%).

```{r}
confint(mod2)
```

```{r}
knitr::kable(ct)
```

Removing these two predictors improve the adjusted R2 value. It has some minor effects on the other parameters, so it would not have been too bad to leave them in.

Earlier it iwas shown that there is a fair amount of correlation between the predictors. To see the effect of this on the model, compute the variance inflation factors.

```{r}
print(vif(mod2))
```

These values are not too large, but `rad` and `tax` are higher than the rest. Furthermore, the basic correlation analysis also showed that tax is strongly correlated with other variables, so investigate the effect of removing it:

(We should be careful since one of the variables is is strongly correlated with was already removed: indus)

```{r}
mod3 <- update(mod2, . ~ . - tax)
summary(mod3)
ct$m03 <- get_mod_stats(mod3)
knitr::kable(ct)
```

Dropping tax had a small effect on the R2 (slight decrease) and a large effect on a number of variables, namely: `rad` and `zn`.

```{r}
print(vif(mod3))
```

The variance inflation factors improved substantially and we should leave tax out of the model.

Do any of the terms interact?

There is a large number of predictors in the model, so evaluating too many interaction terms will cause overfitting. Only consider interaction between the 2 most significant terms for now:

```{r}
mod4 <- update(mod3, . ~ . + lstat:rm)
print(summary(mod4))
```

Interestingly it produces a highly significant term and reduces the importance of the `black` and `zn` terms substansitally. Fit a new model keeping the interaction term, but removing these other two.

```{r}
mod5 <- update(mod4, . ~ . - black - zn)
print(summary(mod5))
```

This improves the R-square value.

```{r}
print(vif(mod5))
```

But produces highly collinear terms - revert back to the model with no interaction terms.

Attempt the non-linear transform of the lstat variable discussed earlier:

```{r}
mod6 <- update(mod3, . ~ . -lstat +  I((1/lstat)^(1/2)))
summary(mod6)
```

Decent improvement and the zn variable is now no longer significant:

```{r}
mod7 <- update(mod6, . ~ . - zn)
summary(mod7)
```

How far have we come?
```{r}
anova(mod, mod3)
```

```{r}
anova(mod, mod7)
```

Pretty far.

```{r}
plot(Boston$medv ~ predict(mod7, Boston))
```

Really good predictions.

## Exercises

### Applied

Number 14 - collinearity

```{r}
set.seed(1)
x1 = runif(100)
x2 = 0.5*x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

```{r}
cor(x1, x2)
plot(x2~x1)
```

```{r}
mod <- lm(y ~ x1 + x2)
summary(mod)
vif(mod)
```

```{r}
mod1 <- lm(y ~ x1)
summary(mod1)
```

```{r}
anova(mod, mod1)
```

```{r}
mod2 <- lm(y ~ x2)
summary(mod2)
```

```{r}
anova(mod, mod2)
```




